{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Topic : Stereo reconstruction and Non-linear optimization\n",
    "\n",
    "#### Instructions\n",
    "<ul>\n",
    "    <li> The second project of the course is designed to get you familiar with stereo reconstruction, and non-linear optimization </li>\n",
    "    <li> Use python for this project. PILLOW and OpenCV are permitted for image I/O. </li>\n",
    "    <li> Submit this notebook as a zipped file on moodle. The format should be $<$team_id$>$_$<$team_ name$>$.zip. Both members have to submit this zip file. </li>\n",
    "    <li> A seperate report is not needed if you're coding in the notebook itself. Please provide adequate descriptions of the approaches you've taken. Also mention work distribution for the two members. </li>\n",
    "    <li> Refer to the late day policy. Start early </li> \n",
    "    <li> Download data from here: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/aryan_sakaria_students_iiit_ac_in/Er5C7351IAlFsvwHUesFeSQBQtlSiAS7AORSEJT2qH_8_w?e=ol98k9  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 1: Stereo dense reconstruction\n",
    "\n",
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo camerasprovide  us  with  a  convenient  way  to  generate  dense  point  clouds.Densehere,  in  contrast  tosparse,means all the image points are used for the reconstruction.  In this part of the assignment you will begenerating a dense 3D point cloud reconstruction of a scene from stereo images.\n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Generate a disparity map for each stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Notethat the images provided are already rectified and undistorted. </li>\n",
    "    <li> Then, using the camera parameters and baseline information generate colored point clouds fromeach disparity map.  Some points will have invalid disparity values, so ignore them.  Use [Open3D]for storing your point clouds. </li>\n",
    "    <li> Register (or transform) all the generated point clouds into your world frame by using the providedground truth poses. </li>\n",
    "    <li> Visualize the registered point cloud data, in color.  Use Open3D for this </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries:\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import normalize \n",
    "import cv2\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transformations(filename='poses.txt'):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    transformation_list = []\n",
    "    for i in range(len(lines)):\n",
    "        transformation_list_temp = lines[i].split()\n",
    "        temp_rot = [] \n",
    "        temp_rot.append( (transformation_list_temp[0:4] ) ) \n",
    "        temp_rot.append( (transformation_list_temp[4:8]  ) ) \n",
    "        temp_rot.append( (transformation_list_temp[8:12]  ) ) \n",
    "        transformation_list.append(temp_rot)\n",
    "    return transformation_list\n",
    "   \n",
    "##### Helper function to transform the 3d points to the first camera frame or the ground truth poses. \n",
    "def newpoint(dp3, x):\n",
    "    \n",
    "    x1 = np.array([\n",
    "        [x[0][0], x[0][1], x[0][2], x[0][3]],\n",
    "        [x[1][0], x[1][1], x[1][2], x[1][3]],\n",
    "        [x[2][0], x[2][1], x[2][2], x[2][3]],\n",
    "    ])\n",
    "    pts = np.mat([dp3[0], dp3[1], dp3[2], 1]).T\n",
    "    mats = np.matmul(x1, pts)\n",
    "    return mats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d reconstruction from stereo image set. \n",
    "\n",
    "#### Code written by Krishna Kumar - 2018122003\n",
    "\n",
    " 1. We first find the disparity map between the 20 image pairs. We used the CV2 inbuilt function stereoBGM() which generated the disparity map. To get a good disparity map we had  to experiment with the parameters of StereoSGBM. \n",
    " \n",
    " 2. Then we generated the Q matrix so that along with the disparity matrix. We can reconstruct 3d points corresponding to the image. \n",
    " \n",
    "    Q=[[1,0,0,-w/2],\n",
    "       [0,-1,0,h/2],\n",
    "       [0,0,0,fl],\n",
    "       [0,0,-bl**(-1),0]]\n",
    "       \n",
    "       The basedline and the K matrix were given in calib.txt.  bl - baseline in m.  w,h - width and height of        the image. fl - focal length of the camera. (obtained from K- Matrix). \n",
    " \n",
    " 3. Then we multiply the Q matrix with the image points agumented with the disparity values (AugMat). So as to recover the 3d point. \n",
    " Each column of the agumented matrix contains contains the [ x y d(x,y) 1] for all the images. \n",
    " \n",
    " 4. After multiplying the image points are converted to point cloud format for visualization. \n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####............Reading the K-matrix and baseline width........#######\n",
    "calibF=open('calib.txt')\n",
    "line=0\n",
    "for lines in calibF:\n",
    "    if(line==1):\n",
    "        k= np.array(lines.strip('\\n').split(' '))\n",
    "    if(line==4):\n",
    "        bDash=lines.split(' ')\n",
    "        b=float(bDash[0])\n",
    "    line+=1\n",
    "kDash=k.astype('float64').reshape(3,3)\n",
    "\n",
    "worldPoints=[]\n",
    "pose_sh = []\n",
    "transformationList=np.array(read_transformations()).astype('float64')\n",
    "pose0 = np.vstack([transformationList[0], np.array([0, 0, 0, 1])])\n",
    "pose_sh.append(pose0)\n",
    "for i in range(1, len(transformationList)):\n",
    "    posey = np.vstack([transformationList[i], np.array([0, 0, 0, 1])])\n",
    "    pose_sh.append(posey)\n",
    "\n",
    "\n",
    "#############................ calcuation of Q matrix ............#####################\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "pathL='img2/'\n",
    "pathR='img3/'\n",
    "\n",
    "fl=7.070912e+02\n",
    "bl = 0.53790448812\n",
    "\n",
    "xc=185 # Centre pixel\n",
    "yc=613 # Centre pixel \n",
    "\n",
    "# calculation of Q m\n",
    "f=k[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# imgLrGB = cv2.cvtColor('/img2/0000000460.png', cv2.COLOR_BGR2RGB)\n",
    "# grayRrGB = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "########################.......... Generating the 3d points for stereo imgage pair.......###################\n",
    "\n",
    "camPoints=[]\n",
    "colours=np.zeros((370, 1226, 3))\n",
    "thdoutputs=[]\n",
    "thdcolours=[]\n",
    "imgPointMap = {}\n",
    "imgPointMap_alt = {}\n",
    "imgPointMap_2 = {}\n",
    "\n",
    "NumOfPairs=5 # Change this paramter to include more images. \n",
    "\n",
    "for i in range(NumOfPairs):\n",
    "    \n",
    "    # Reading the images. \n",
    "    img2=pathL+'0'*7+str(460+i)+str('.png')\n",
    "    img3=pathR+'0'*7+str(460+i)+str('.png')\n",
    "    \n",
    "\n",
    "    imgLF=cv2.imread(img2)\n",
    "    imgRF=cv2.imread(img3)\n",
    "    \n",
    "    imgL=cv2.cvtColor(imgLF,cv2.COLOR_BGR2RGB)\n",
    "    imgR=cv2.cvtColor(imgRF,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "#########........Use this code to downsample the image if needed...........############\n",
    "\n",
    "    scale_percent = 100 # percent of original size\n",
    "    width = int(imgL.shape[1] * scale_percent / 100)\n",
    "    height = int(imgR.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height) \n",
    "    h, w = imgL.shape[:2]\n",
    "#     print(h)\n",
    "\n",
    "\n",
    "###########............. consturction of Q matrix.................##############\n",
    "\n",
    "    numOfPix= width*height\n",
    "    Q=np.array([[1,0,0,-w/2],[0,-1,0,h/2],[0,0,0,fl],[0,0,-bl**(-1),0]])\n",
    "    Q=Q.astype('float64')\n",
    "    \n",
    "#     imgL=cv2.resize(imgLF, dim, interpolation = cv2.INTER_jnreconAREA) \n",
    "#     imgR=cv2.resize(imgRF, dim, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "    window_size = 3 \n",
    "    \n",
    "    minDisp = 16\n",
    "    numDisp = 112 - minDisp\n",
    "    \n",
    "###########...Finding disparity map for the stereo pair...########\n",
    "    \n",
    "    ImageDisp = cv2.StereoSGBM_create(\n",
    "        minDisparity=minDisp,             \n",
    "        blockSize=7,\n",
    "        numDisparities=numDisp + (2) * 16,\n",
    "        P1=8*3*window_size**2,    \n",
    "        P2=32*3*window_size**2,\n",
    "        uniquenessRatio=12,\n",
    "        speckleWindowSize=400,\n",
    "        disp12MaxDiff=1,\n",
    "        speckleRange=5,\n",
    "        #preFilterCap=63\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dispMat = ImageDisp.compute(imgL, imgR).astype(np.float32)\n",
    "#     print(dispMat)\n",
    "    \n",
    "##############...product of Q matix and augumented image points.......#############\n",
    "### Accumiliating in array thdoutputs and thdcolurs from the image to generated point coud ####\n",
    "\n",
    "    camPoints = np.zeros((370,1226,3))\n",
    "    colours = np.zeros((370,1226,3))\n",
    "    \n",
    "    imgPointMap[i] = []\n",
    "    imgPointMap_alt[i] = []\n",
    "    imgPointMap_2[i] = []\n",
    "    for k in range(len(imgL)):\n",
    "        for j in range(len(imgL[k])):\n",
    "            colours[k][j] = imgL[k][j]\n",
    "            pt_temp = Q @ np.array([k, j, dispMat[k][j]/16.0, 1]).T\n",
    "            W = pt_temp[3]\n",
    "            to_ap = [pt_temp[0]/W, pt_temp[1]/W, pt_temp[2]/W]\n",
    "            camPoints[k][j] = to_ap\n",
    "            imgPointMap[i].append(([k, j], to_ap))\n",
    "            if dispMat[i][j] > dispMat.min():\n",
    "                imgPointMap_alt[i].append(([k, j], to_ap))\n",
    "                # This step takes the above project 3d, and transforms them according to the ground truth poses.\n",
    "                pt_new_temp = np.array(newpoint(to_ap, pose_sh[i]).T).astype('float32')[0]\n",
    "                pt_add = [pt_new_temp[0], pt_new_temp[1], pt_new_temp[2]]\n",
    "                imgPointMap_2[i].append(([k, j], pt_add))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # applying mask for better results. And appending the 3d points for all the image pairs. \n",
    "    mask_map = dispMat > dispMat.min()\n",
    "    o_camPoints = camPoints[mask_map]\n",
    "    o_colours = colours[mask_map]\n",
    "    o_colours =(o_colours/255).astype('float64')\n",
    "    for j,pts in enumerate(o_camPoints):\n",
    "        thdoutputs.append(np.array(newpoint(pts, pose_sh[i]).T).astype('float32')[0])\n",
    "        thdcolours.append(o_colours[j])\n",
    "        \n",
    "thdoutputs = np.array(thdoutputs)\n",
    "thdcolours = np.array(thdcolours)\n",
    "\n",
    "#print(thdoutputs)\n",
    "# print(thdcolours.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################............. Visualizing the point cloud ...................#####################\n",
    "accPntCld=[]\n",
    "pntCld= o3d.geometry.PointCloud()\n",
    "pntCld.points= o3d.utility.Vector3dVector(thdoutputs)\n",
    "pntCld.colors= o3d.utility.Vector3dVector(thdcolours)\n",
    "o3d.io.write_point_cloud(\"all_pcs.ply\", pntCld)\n",
    "o3d.visualization.draw_geometries([pntCld])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 2: Motion estimation using iterative PnP\n",
    "\n",
    "###### Coded by Pulkit Gera (Roll no: 20171035)\n",
    "\n",
    "Using the generated reconstruction from the previous part, synthesize a new image taken by a virtualmonocular camera fixed at any arbitrary position and orientation.  Your task in this part is to recoverthis pose using an iterative Perspective-from-n-Points (PnP) algorithm. \n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Obtain a set of 2D-3D correspondences between the the image and the point cloud.  Since hereyou’re generating the image, this should be easy to obtain. </li>\n",
    "    <li> For this set of correspondences compute the total reprojection error c= $\\sum_{i} ‖x_i−P_{k}X_i‖^2 $    where $P_{k}= K[R_{k}|t_{k}]$, $X_{i}$ is the 3D point in the world frame, $x_{i}$ is its corresponding projection. </li>\n",
    "    <li> Solve for the pose $T_{k}$ that minimizes this non-linear reprojection error using a Gauss-Newton (GN)scheme.  Recall that in GN we start with some initial estimated value $x_{o}$ and iteratively refine the estimate using $x_{1}$= $∆x+x_0$, where $∆x$ is obtained by solving the normal equations $J^{T}J∆x$= -$J^{T}e$, until convergence.The main steps in this scheme are computing the corresponding Jacobians and updating the estimates correctly.  For our problem,  use a 12×1 vector parameterization for $T_{k}$(the top 3×4submatrix).  Run the optimization for different choices of initialization and report your observations. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct R|t Matrix \n",
    "Q = np.array([ [-9.1e-01, 5.5e-02, -4.2e-01, -1.9e+02],\n",
    "               [4.2e-02, 9.983072e-01, 4.2e-02, 1.7e+00],\n",
    "               [4.2e-01, 2.1e-02, -9.2e-01, 5.5e+01],\n",
    "                 [0,0,0,1]])\n",
    "Q=np.linalg.inv(Q)[:-1]\n",
    "\n",
    "#Calib Matrix \n",
    "K = kDash\n",
    "\n",
    "# Correct projection matrix, the one we should be getting after GN \n",
    "P = K@Q # 3x4 matrix \n",
    "print(\"Correct projection matrix: \\n\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.randint(low=0,high=len(imgPointMap_alt[0]),size=100)\n",
    "pts_3d = []\n",
    "gt_pts_2d = []\n",
    "pts_3d = [imgPointMap_alt[0][i][1] for i in range(0,10000,100)]\n",
    "pts_3d = np.asarray(pts_3d)\n",
    "pts_3d = np.hstack((pts_3d,np.ones((100,1))))\n",
    "gt_pts_2d = (P@(pts_3d.T)).T\n",
    "for i in range(100):\n",
    "    gt_pts_2d[i] /= gt_pts_2d[i,-1]\n",
    "print(gt_pts_2d.shape,pts_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton \n",
    "- Now we run the gauss newton algorithm and try to refine P with every iteration. \n",
    "- We take an initial estimate of P that is close to the actual value of P, because otherwise, GN would get stuck in local minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_est = np.array([ [-8.8e+02,5.4e+01,-2.4e+02,-1.5e+05],\n",
    "                   [-3.6e+01,7.1e+02,-1.4e+02,1.4e+02],\n",
    "                   [-4.1e-01,4.1e-02,-8.9e-01,-2.9e+01] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now running Gauss Newton algorithm. With every iteration, we try to find a better estimate for P\n",
    "- Please refer to the handwritten notes attached along with submission about Jacobian\n",
    "- Every iteration, we do P + delP, calculated from jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Gauss newton: \\n \")\n",
    "while True: \n",
    "    pts_2d = [] # corresponding 2D location estimated based on our P matrix \n",
    "    J = []\n",
    "    curr_pts_2d = (P_est@(pts_3d.T)).T\n",
    "    for i in range(100):\n",
    "        curr_pts_2d[i] /= curr_pts_2d[i,-1]\n",
    "    residual = gt_pts_2d-curr_pts_2d\n",
    "    residual = residual[:,:-1].flatten()\n",
    "    objective = residual.T@residual\n",
    "     \n",
    "    print(\"squared sum error:\", objective)\n",
    "    if(objective < 0.0001):\n",
    "        break\n",
    "        \n",
    "    for i in range(pts_3d.shape[0]):\n",
    "        X_num,Y_num,Z_denom = P_est@pts_3d[i]\n",
    "        X_pt,Y_pt,Z_pt,_ = pts_3d[i]\n",
    "        \n",
    "        J.append(np.array([X_pt/Z_denom,Y_pt/Z_denom,Z_pt/Z_denom,1/Z_denom,0,0,0,0,\n",
    "                             (-X_num*X_pt)/(Z_denom**2),(-X_num*Y_pt)/(Z_denom**2),(-X_num*Z_pt)/(Z_denom**2),-1/(Z_denom**2)]))\n",
    "        J.append(np.array([0,0,0,0,X_pt/Z_denom,Y_pt/Z_denom,Z_pt/Z_denom,1/Z_denom,\n",
    "                             (-Y_num*X_pt)/(Z_denom**2),(-Y_num*Y_pt)/(Z_denom**2),(-Y_num*Z_pt)/(Z_denom**2),-1/(Z_denom**2)]))\n",
    "    J = np.asarray(J).reshape(200,12)\n",
    "    H = J.T@J\n",
    "    L = np.linalg.pinv(H)\n",
    "    update = J.T@residual\n",
    "    delP = (L@update).reshape(3,4)\n",
    "    P_est = P_est+delP\n",
    "\n",
    "print(\"Predicted projection matrix after Gauss Newton: \\n\")\n",
    "print(P_est)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the algorithm converges after a few steps. This is the case for all the images we are running it on.\n",
    "\n",
    "1. We already have the 3D points generated from the previous part, which we used to render the point cloud. We, thereafter, find the 2D pixel points that correspond to these 3D points. We do so by multiplying each 3D point by the projection matrix, obtained from R, K, t. K has been given to us and R|t we get from the poses.txt.\n",
    "\n",
    "\n",
    "2. We take inverse of R|t matrix because we want to go from world to camera. Hence, we add a new row to make it 4X4. Basically, P = KRt\n",
    "\n",
    "\n",
    "3. After this is done, we find the actual 2D points that correspond to the 3D point by multiplying each 3D point with P.\n",
    "\n",
    "\n",
    "4. Once this is complete, we run the Gauss Newton algo and try to refine P with every iteration. We take an intial estimate of P that is close to the actual value of P, because otherwise, we shall be stuck around a local minima. \n",
    "\n",
    "\n",
    "5. Now, running Gauss Newton algo, with every iteration, we try to find a better estimate for P. Every iteration, we do a P + delP calculated from the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-B Project -2 \n",
    "\n",
    "##### Answered by Pulkit Gera (20171035) and Krishna Kumar together by discussion. \n",
    "\n",
    "\n",
    "1. Basics - How do we know this (camera_ind) information in practical setting? Explain.\n",
    "\n",
    "Answer - In the Structre for motion (SFM) pipeline, In the front end, intially mutiple images of an object are being taken . From these multiple images, common features are detected from consequtive image sets (using methods like SIFT or ORB and so on). After we have detected these feature points, the fundamental matrix of the image points are detected. (Using algorithms like the eight point algorithm). From which we can find the information between the points like Rotation and translation matrices. Once we know the Rotation and translation matrices. We can find out reconstruct the 3d points world points by considering by using methods like pnp. \n",
    "\n",
    "2. Basics - How do we know this (point_ind) information in practical setting? Explain.\n",
    "\n",
    "Indexing to 2D points and labelling to 3D points helps in tracking which 3D points generated by 2D points. Similar is answered for previous question. Initial estimates for 3D points is done by triagnulation and backprojection of 2D points.\n",
    "\n",
    "3. Transformations - project() function: In the project() function, would it make any difference if I do translate first, then rotate? Why/why not?\n",
    "\n",
    "Project function is basically roatating from the world frame to local frame and translating i.e shift the origin with local camara frame. This projection is required to transform 3D points projected in aribitary world frame to local carama frame. The translation or shifting vector is in local camara frame where as the projected 3D points are in different frame. It is not possible to use the translation vector of different frames. Hence, it is required to bring the projected 3D points first to local camara frame by rotation to ensure that rotated projection and translation vector are in same frame. \n",
    "update answer 3 with However, in this case it does not, and the output is **exactly the same* regardless of the order , this happens because the translation is along the direction of the rotation axis in the project function and thus the result is same regardless of order.\n",
    "\n",
    "4. m above is not \"MN\" (2) unlike our lecture notes. Why is that so? (bundle adjustment sparsity function). \n",
    "\n",
    "Ans: loss function measures the deviation between observed 2D points and reprojected 2D points. 2D points contribtutes to two resideual points as two cooridnates. Hence, jacobian here contributes two residuals, unlike in the class. In class it is assumed that, M images contributes N points. So, M*N 2D points were considered.\n",
    "\n",
    "5. Why are we doing n_cameras * 9 here instead of n_cameras * 12? Recollect :Every i. individual motion Jacobian was (1)12 in our lecture notes. \n",
    "\n",
    "Ans: This number represents the number of camara parameters considered for optmization to estimate the 3D world points. Unlike in class, here we considered only 9 camara parameters viz., 3 for translation, 3 for rotation, 2 for distortion and one for focal length. Hence n_camaras9 instead of n_camaras*12. \n",
    "\n",
    "6. Explain what you understand from above 6 lines of code by coding? (bundle adjustemnt sparsity function). \n",
    "\n",
    "Ans: First for loop sets the 2 lines of 9 colums of jacobian matrix correpsonds to 9 camara parameters set to 1. This is done every two rows at a time. In a similar way, second for loop sets three colums of world frame of ith observation sets to one. Rest of the columns of other observation are left to zero. This way required sparsity is achieved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
